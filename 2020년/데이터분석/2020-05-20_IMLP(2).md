## IMLP (2) - Supervised Learning

- Regression, Classification
- Generalization, Overfitting, Underfitting

[Colab code - IMLP(2)](https://colab.research.google.com/drive/1xsOdR-aVVOiImimhQ9JiEUQcmaf4fTzV)

#### 유방암 데이터 이용하기

```python
import numpy as np
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()

print('target : {}'.format(
    {target : count for target, count in zip(cancer.target_names, np.bincount(cancer.target))}))
# target : {'malignant': 212, 'benign': 357}
```

​	타겟 데이터가 유방암 데이터처럼 불균형한 비를 이룰 때는 제대로 된 학습 및 테스트가 불가능하다. **이처럼 구성비가 다른 불균형 데이터를 다룰 때에 `train_test_split` 함수에서 `stratify=y_data` 옵션을 사용**해준다.

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    cancer.data, cancer.target, stratify=cancer.target, random_state=1)
```

##### 최근접 이웃 (K-NN)	

​	최근접 이웃 알고리즘은 가까운 이웃들의 값을 이용하여 예측하는 방법이다. 따라서 예측시 살펴볼 이웃들의 갯수에 따라 예측값이 변할 수 있다.

```python
import matplotlib.pyplot as plt
from sklearn import neighbors

training_acc = []
test_acc = []

neighbors_settings = range(1,11)

for n_neighbors in neighbors_settings:
  clf = neighbors.KNeighborsClassifier(n_neighbors = n_neighbors)
  clf.fit(X_train, y_train)

  training_acc.append(clf.score(X_train, y_train))
  test_acc.append(clf.score(X_test, y_test))

plt.plot(neighbors_settings, training_acc, label='training accuracy')
plt.plot(neighbors_settings, test_acc, label='test accuracy')

plt.ylabel('Accuracy')
plt.xlabel('n_neighbors')
plt.legend()
```



##### KNN Regressor

​	KNN 알고리즘을 이용하여 회귀 문제를 분석할 수도 있다. sklearn의  `KNeighborsRegressor`에서 구현되어 있다. 이웃들의 평균값을 예측값으로 사용한다.

```python
reg = neighbors.KNeighborsRegressor(n_neighbors=9)
reg.fit(X_train, y_train)
reg.score(X_test, y_test)
```

	> while the nearest k-neighbors algorithm is easy to understand, it is not often used in practice, due to prediction being slow and its inability to handle many features. 
	>
	> 최근접 이웃 알고리즘은 이해하기 쉽지만, 실제 예측에는 잘 사용되지 않는다. 예측 속도가 아주 느리고, 다양한 수의 피쳐를 핸들링하기에 좋지 않다.

#### Boston 데이터 이용하기

​	회귀 문제를 분석할 때는 보통 Linear, Ridge, Lasso regression 모형을 사용한다.

```python
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

boston = load_boston()
X_train, X_test, y_train, y_test = train_test_split(
    boston.data, boston.target, random_state=65)
```

##### linear regression

```python
from sklearn.linear_model import LinearRegression
lr = LinearRegression().fit(X_train, y_train)

print('Training set score : {:.2f}%'.format(lr.score(X_train, y_train)))
print('Test set score : {:.2f}%'.format(lr.score(X_test, y_test)))
```

##### ridge regression

```python
from sklearn.linear_model import Ridge
ridge = Ridge().fit(X_train, y_train)

print('Training set score : {:.2f}%'.format(ridge.score(X_train, y_train)))
print('Test set score : {:.2f}%'.format(ridge.score(X_test, y_test)))
```

##### lasso regression

```python
from sklearn.linear_model import Lasso
lasso = Lasso().fit(X_train, y_train)

print('Training set score : {:.2f}%'.format(lasso.score(X_train, y_train)))
print('Test set score : {:.2f}%'.format(ridge.score(X_test, y_test)))
print('Number of features used: {}'.format(np.sum(lasso.coef_ !=0)))	# 10
```

##### ridge, lasso regression with alpha

​	릿지, 라소 회귀 모형에는 정규화를 위한 파라미터인 alpha 값이 있다. (default 1.0)  이 값을 이용하여 사용하는 계수와의 결합도를 조정할 수 있다. alpha 값이 0에 가까워질수록 (계수 결합도를 강하게 가져갈수록) linear regression 모델과 비슷한 양상을 보인다. lasso regression의 경우, alpha 값을 줄일수록(데이터를 좀 더 적합시킬수록) 사용하는 계수가 늘어나는 것을 확인할 수 있다. Train / Test 정확도를 확인하면서 과대/과소 적합에 유의하여 적절한 값을 찾아가는 것이 중요하다.

> **A lower alpha allowed us to fit a more complex model**, which worked better on the training and test data.

```python
# ridge,lasso regression set alpha

from sklearn.linear_model import Ridge, Lasso
ridge = Ridge(alpha=0.001).fit(X_train, y_train)

print('Training set score : {:.2f}%'.format(ridge.score(X_train, y_train)))
print('Test set score : {:.2f}%'.format(ridge.score(X_test, y_test)))

lasso = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)
print('Training set score : {:.2f}%'.format(lasso.score(X_train, y_train)))
print('Test set score : {:.2f}%'.format(ridge.score(X_test, y_test)))
print('Number of features used: {}'.format(np.sum(lasso.coef_ !=0)))	# 13
```

#### 다시 유방암 데이터  (로지스틱 회귀)

##### LinearSVC, Logistic Regression

​	분류를 위한 로지스틱 회귀 모델에는 LinearSVC(SVM), Logistic Regression 등이 있다. sklearn에서 제공하는 유방암 판별 데이터셋은 이 모델을 사용할 때 0.92% 이상의 정확도를 가진다.

```python
for model in [LinearSVC(), LogisticRegression()]:
    clf = model.fit(X_train, y_train)
    print('{:.2f}%'.format(clf.score(X_train,y_train)))
    print('{:.2f}%'.format(clf.score(X_test,y_test)))
```

​	이 분류 모델들은 C라고 하는 trade-off 파라미터를 가지고 있는데, 정규화와 관련이 있다. 높은 C 값을 가질수록 좀 더 데이터에 fit 한 데이터를 가지게 된다. (낮은 정규화)

```python
logreg100 = LogisticRegression(C=100).fit(X_train, y_train)
print('{:.2f}%'.format(logreg100.score(X_train,y_train)))	# 0.95%
print('{:.2f}%'.format(logreg100.score(X_test,y_test)))	# 0.93%

logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)
print('{:.2f}%'.format(logreg001.score(X_train,y_train)))	# 0.94%
print('{:.2f}%'.format(logreg001.score(X_test,y_test)))	# 0.94%
```

```python
plt.plot(logreg100.coef_.T, '^', label="C=100")
plt.plot(logreg001.coef_.T, 'v', label="C=0.001")
plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)
plt.hlines(0, 0, cancer.data.shape[1])
plt.ylim(-5, 5)
plt.xlabel("Coefficient index")
plt.ylabel("Coefficient magnitude")
```

​	모델 계수는 30개, 경계선은 1개의 직선으로 모델 생성됨.

```python
linear_svm = LinearSVC().fit(X_train, y_train)
print('Coefficient shape : ', linear_svm.coef_.shape)
print('Intercept shape : ', linear_svm.intercept_.shape)

# Coefficient shape :  (1, 30)
# Intercept shape :  (1,)
```









