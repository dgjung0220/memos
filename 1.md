### Ian Goodfellow book review (Regularization part)

---
1. Parameter norm penalties
2. Dataset augmentation
3. Noise Robustness : to input, weights and output
4. Semi-Supervised learning = learning a representation
5. Multitask learning
6. Early stopping
7. Parameter tying and Parameter sharing
8. Sparse representation
9. Bagging and other Ensemble methods
10. Dropout
11. Adversarial training
---

#### 1. Parameter Norm penalites
Many regularization approaches are based on limiting the capacity of models by adding a parameter norm penalty to the objective function.(weight를 제한하기 위해 penalty 를 더함)

- L2 parameter regularization
- L1 parameter regularization

#### 2. Dataset augmentation
- The best way to make a machine learning model generalize better is to train it on more data.
- Of cousre, in practice, the amount of data we have is limited. One way to get around this problem is to create fake data and add it to the training set.
- Label preserving transformation is used.
- Injecting noise in the input to a neural network can also be seen as a form of data augmentation.
- Often, hand-designed dataset augmentation schemes can dramatically reduce the generalization error of a machine learning technique.

#### 3. Noise Robustness
- In general case, it is important to remember that noise injection can be much more powerful than simply shrinking the parameters, especially when the noise is added to the hidden units.
- Another way that noise has been used is by adding it to the weights. (dropconnect)
- Most datasets have some amount of mistakes in the ylabels. Label smoothing can be used in this regard. (라벨을 부드럽게, [1 0 0] -> [0.8 0.2 0.2])

#### 4. Semi-Supervised Learning