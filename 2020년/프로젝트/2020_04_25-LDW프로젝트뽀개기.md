#### Lane Departure Warning 이해하기



​	lane detection,  해외 유튜브 방송에 관련 소개도 많고, udacity 에 강의도 잘 되어 있어 관련 소스를 찾는 건 어렵지 않다. 하지만 한글로 정리된 내용은 거의 없다. 그래서 그냥 내가 보려고 정리하는 lane departure warning 메모.



#### 살펴 볼 소스코드

​	여러 소스들을 검색하면서 내가 찾은 것은 https://github.com/JunshengFu/driving-lane-departure-warning, 현재 스웨덴의 Zenuity 에서 자율주행 엔지니어로 일하고 계시는 Junsheng Fu라는 분. 이 프로젝트 외에도 Junsheng Fu github에 참고할만한 다양한 프로젝트들이 있다. 별 박아주자.



#### 환경 설정

소스 클로닝 후, conda 를 이용하여 개발환경을 셋팅한다.

```shell
conda env create -f environment-gpu.yml
```

​	리눅스 환경에서 설정된 프로젝트인지 윈도우에서 하려니 'conflict error' 가 발생했다. 처음엔 뭐가 문제인지 이리 저리 살펴보다가 리눅스 환경에서 하니 잘 된다... 윈도우즈에선 필요하다고 하는 거 잘 받아서 셋팅하자. 잘 되더라.



#### python 버젼 차이로 생기는 에러 몇몇 고치기

> **TypeError: slice indices must be integers or None or have an __index__ method**
>
> 나눈 값이 정수가 아니라서 생기는 에러로, / 대신 // 로 수정. (이 전 버젼에서는 / 로 나누어도 정수 반환)
>
> 그 외에 float라 생기는 몇몇 문제들을 고치고 돌리면, 코드는 잘 돌아간다.



#### 개요

​	이 프로젝트는 총 6개의 파일 및 폴더로 이루어져있다.

- calibration.py : 카메라를 보정하는 소스와 보정 결과 저장.
- main.py : 2가지 데모에 대한 메인 코드 (1 : 스틸샷(이미지) 한 장에 대한 LDW 결과를 matplotlib 을 통해 보여줌 / 2 : moviepy를 이용하여 LDW 결과를 동영상에 저장하여 반환)
- lane.py : lane class, lane 을 인지하고 표시하는 대부분의 역할이 들어있겟죠?
- camera_cal : 카메라 보정 결과와 카메라 보정에 필요한 사진들
- examples : 샘플 이미지 및 비디오
- enviroment-gpu.yml : GPU 환경에 필요한 환경 설정 파일



##### Run source code

demo 돌리고 싶으면, 

```shell
python main.py
```

내 카메라로 셋팅해서 해보고 싶으면 캘리브레이션 필요,

```shell
python calibration.py
```



#### 단계별로 살펴보기

1. Camera calibration
   - 카메라 보정을 위한 chessboard 이미지셋을 모은다.
   - Camera calibration matrix / distortion coefficients 계산
   - 들어오는 이미지에 distortion correction 적용
2. Lane detection / tracking
   - 색상 변환 (color transform), 그래디언트 (Gradient) 등을 사용하여 이진 이미지 만들기
   - Perspective Transform 을 이용하여 이미지 정 변환 (bird-eye view 만들기)
   - 차선 바운더리 영역을 찾기 위해 차선을 인지 및 추적
3. Lane status anlysis (차선 상태 분석)
   - 차선 곡률 (curvature of the lane) 구하기
   - Center-line offset 구하기 (예상되는 센터값에서 얼마나 떨어져있는지?)
4. Lane augumentation
   - 인지한 차선 바운더리를 다시 오리지널 이미지에 입혀 되돌린다.
   - 이미지에 도로 상태 표시 출력.

---

#### Camera calibration

**Camera calibration matrix / distortion coefficients 계산하기**

​	카메라 캘리브레이션에 대한 내용은 모두 *calibration.py* 에 있다. 먼저 오브젝트 포인트(Object point) 를 준비하는데 이 포인트는 chessboard 판의 코너 점의 월드 좌표가 된다. 여기서 체스보드가 z=0 으로 (x,y) 평면에 고정되어 있다고 가정하면, 오브젝트 포인트들은 각 보정 이미지와 동일하다. 따라서 ``objp`` 는 복사된 좌표의 배열이며, 테스트 이미지에서 모든 체스판 모서리를 성공적으로 감지할 때마다 ``objpoints``는 추가된다. ``imgpoint``에는 체스보드 디텍션이 성공함에 따라 이미지 평면상의 (x,y) 픽셀좌표값이 저장된다.

![undist_example.jpg](https://github.com/JunshengFu/driving-lane-departure-warning/blob/master/examples/undist_example.jpg?raw=true)

​	읽어도 무슨 말인지 이해가 안 된다. 코드 설명이니 당연하다. calibration.py 코드를 보면서 이해해보자*  ([[Computer Vision] Calibration.py 소스코드 살펴보기](https://dgjung.me/영상처리/2020/05/06/calibration_code/) 참고)

---

#### Pipeline to process image and video

**lane.py 의 함수 process_frame() 살펴보기**

1. Apply the distortion correction (왜곡 정정)

   ```python
   # resize the input image according to scale
   img_undist_ = cv2.undistort(img, mtx, dist, None, mtx)
   img_undist = cv2.resize(img_undist_, (0,0), fx=1/input_scale, fy=1/input_scale)
   ```

   ​	calibration(cv2.calibrateCamera()) 과정을 통해 얻은 mtx, dist (행렬 및 왜곡계수) 를 이용하여 왜곡 보정한 이미지를 만든다.

   [![alt text](https://github.com/JunshengFu/driving-lane-departure-warning/raw/master/examples/undistortion.jpg)](https://github.com/JunshengFu/driving-lane-departure-warning/blob/master/examples/undistortion.jpg)

2. Use color and gradients filters to get a binary image (이진 이미지 변환을 위한 필터 이용)

   ```python
   def find_edges(img, s_thresh=s_thresh, sx_thresh=sx_thresh, dir_thresh=dir_thresh):
   
       img = np.copy(img)
       # Convert to HSV color space and threshold the s channel
       hls = cv2.cvtColor(img, cv2.COLOR_BGR2HLS).astype(np.float)
       s_channel = hls[:,:,2]
       s_binary = threshold_col_channel(s_channel, thresh=s_thresh)
   
       # Sobel x
       sxbinary = abs_sobel_thresh(img, orient='x', sobel_kernel=3, thresh=sx_thresh)
       # mag_binary = mag_thresh(img, sobel_kernel=3, thresh=m_thresh)
       # # gradient direction
       dir_binary = dir_threshold(img, sobel_kernel=3, thresh=dir_thresh)
       #
       # # output mask
       combined_binary = np.zeros_like(s_channel)
       combined_binary[(( (sxbinary == 1) & (dir_binary==1) ) | ( (s_binary == 1) & (dir_binary==1) ))] = 1
   
       # add more weights for the s channel
       c_bi = np.zeros_like(s_channel)
       c_bi[( (sxbinary == 1) & (s_binary==1) )] = 2
   
       ave_binary = (combined_binary + c_bi)
   
       return ave_binary
   ```

   ​	이미지는 먼저 HLS  색상 모델로 변환하며, S(Saturation, 채도) 값을 이용하여 이미지를 필터링한다. 그 결과 다양한 조명 조건에서도 노란색, 흰색 등의 차선을 찾을 수 있다.  그 다음 sobel x 필터와 gradient direction 필터를 이용하여 수평선을 최대한 제거한다. 마지막으로, 두 개 이상의 후보 차선이 발견되는 경우를 처리하기 위해 그라디언트 필터보다 S 채널 필터에 가중치의 두 배를 할당한다. 결과적으로 노란색 차선이 도로 가장자리보다 더 잘 보이게 된다.

   [![alt text](https://github.com/JunshengFu/driving-lane-departure-warning/raw/master/examples/binary.png)](https://github.com/JunshengFu/driving-lane-departure-warning/blob/master/examples/binary.png)

3. Use perspective transform to see the image in bird-view (버드뷰 구현을 위한 Perspective Transform 알고리즘 사용하기)

   ```python
   def warper(img, M):
   
       # Compute and apply perspective transform
       img_size = (img.shape[1], img.shape[0])
       warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_NEAREST)  # keep same size as input image
   
       return warped
   ```

   ​	먼저 주행 이미지의 차선에서 사다리꼴의 네 개의 꼭지점을 선택하고, opencv 내장함수인 warpPerspective 함수를 이용하여 원근 변환하여 계산된 네개의 꼭지점을 얻는다.

   [![alt image](https://github.com/JunshengFu/driving-lane-departure-warning/raw/master/examples/warper.png)](https://github.com/JunshengFu/driving-lane-departure-warning/blob/master/examples/warper.png)

4. Identify lane-line pixels and fit their positions with a polynomial (차선을 인식하고 다항식을 이용하여 위치를 정확히 하기)

   ​	입력 이미지가 단일 혹은 비디오의 첫 번째 프레임일 경우, detector 함수를 사용한다. detector 함수에서는 다음과 같이 이미지를 처리한다.

   ```python
   def detector(binary_sub, ploty, visualization=False):
   
       left_fit, right_fit = full_search(binary_sub, visualization=visualization)
   
       left_fitx = left_fit[0] * ploty**2 + left_fit[1] * ploty + left_fit[2]
       right_fitx = right_fit[0] * ploty**2 + right_fit[1] * ploty + right_fit[2]
       std_value = np.std(right_fitx - left_fitx)
       if std_value < (85 /input_scale):
           left_lane.current_poly = left_fit
           right_lane.current_poly = right_fit
           left_lane.cur_fitx = left_fitx
           right_lane.cur_fitx = right_fitx
           left_lane.detected = True
           right_lane.detected = True
       else:
           left_lane.current_poly = left_lane.prev_poly
           right_lane.current_poly = right_lane.prev_poly
           if len(left_lane.prev_fitx) > 0:
               left_lane.cur_fitx = left_lane.prev_fitx[-1]
               right_lane.cur_fitx = right_lane.prev_fitx[-1]
           else:
               left_lane.cur_fitx = left_fitx
               right_lane.cur_fitx = right_fitx
           left_lane.detected = False
           right_lane.detected = False
   ```

   - 먼저 이진 warped 이미지의 아래쪽 절반에 대해 열 히스토그램(히스토그램이라고 해서 이해하기 어려웠는데, 행을 기준으로 한 sum이다. ``  histogram = np.sum(binary_warped[binary_warped.shape[0]//2:,:], axis=0)``)을 생성한다. 이 히스토그램에서 가장 눈에 띄는 두 피크는 차선 밑면의 x 위치를 나타내는 좋은 지표다. 이 것을 차선을 검색할 곳의 출발점으로 사용할 수 있다.

     [![alt image](https://github.com/JunshengFu/driving-lane-departure-warning/raw/master/examples/hist.png)](https://github.com/JunshengFu/driving-lane-departure-warning/blob/master/examples/hist.png)

   - 위의 방법으로 얻은 좌표를 기점으로 차선 중심으로부터 주위의 선들을 찾기 위해 sliding window 를 사용한다. 하단에서 시작하여 상단까지 찾는다. 다음 그림과 같이 시각화된다.

     [![img](https://github.com/JunshengFu/driving-lane-departure-warning/raw/master/examples/windows.png)](https://github.com/JunshengFu/driving-lane-departure-warning/blob/master/examples/windows.png)

   - ployfit 함수를 이용하여 차선을 정확히 맞춘다.

     * *ployfit* : 데이터에서 원하는 차수의 데이터를 찾는다. (세번째 인자: 찾고자 하는 함수의 차수)

     ```python
      # Fit a second order polynomial to each
      left_fit = np.polyfit(lefty, leftx, 2)
      right_fit = np.polyfit(righty, rightx, 2)
     ```

5. Calculate the radius of curvature of the lane and the position of the vehicle with respect to center. (차선의 곡률 반경과 중심을 기준으로 차량의 위치를 계산)

   - **Radius of curvature (곡률 반경)**

     ​	특정 지점에서 곡선의 곡률 반경은 근사 원의 반경으로 정의된다. (참고 : herehttp://www.intmath.com/applications-differentiation/8-radius-curvature.php)

     [![img](https://github.com/JunshengFu/driving-lane-departure-warning/raw/master/examples/formula.png)](https://github.com/JunshengFu/driving-lane-departure-warning/blob/master/examples/formula.png)

     ​	`lane.py`에서 `measure_lane_curvature` 으로 해당 내용이 구현되어 있다.

     ```python
     def measure_lane_curvature(ploty, leftx, rightx, visualization=False):
     
         leftx = leftx[::-1]  # Reverse to match top-to-bottom in y
         rightx = rightx[::-1]  # Reverse to match top-to-bottom in y
     
          # choose the maximum y-value, corresponding to the bottom of the image
         y_eval = np.max(ploty)
     
         # Define conversions in x and y from pixels space to meters
         ym_per_pix = 30/(frame_height/input_scale) # meters per pixel in y dimension
         xm_per_pix = LANEWIDTH/(700/input_scale) # meters per pixel in x dimension
     
         # Fit new polynomials to x,y in world space
         left_fit_cr = np.polyfit(ploty*ym_per_pix, leftx*xm_per_pix, 2)
         right_fit_cr = np.polyfit(ploty*ym_per_pix, rightx*xm_per_pix, 2)
         # Calculate the new radii of curvature
         left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])
         right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])
         # Now our radius of curvature is in meters
         # print(left_curverad, 'm', right_curverad, 'm')
     
         if leftx[0] - leftx[-1] > 50/input_scale:
             curve_direction = 'Left curve'
         elif leftx[-1] - leftx[0] > 50/input_scale:
             curve_direction = 'Right curve'
         else:
             curve_direction = 'Straight'
     
         return (left_curverad+right_curverad)/2.0, curve_direction
     ```

   - **Vehicle position (차량 위치 산정)**

     ​	카메라가 대략 전면 창의 중앙에 장착되어 있다고 가정하고, 차선의 위치를 계산했다.  왼쪽과 오른쪽 차선의 맨 아래 위치를 가져와서 이미지 프레임의 중간 부분(차량)과 비교하였고, 실제 거리에서 거리를 추정하기 위해 왼쪽 차선과 오른 쪽 차선 사이의 너비 (예: 미국의 경우 3.7m) 에 대한 사전 지식을 활용하였다. `lane.py` 에서 `off_center` 라는 함수로 해당 내용이 구현되어 있다.

     ```python
     def off_center(left, mid, right):
         """
     
         :param left: left lane position
         :param mid:  car position
         :param right: right lane position
         :return: True or False, indicator of off center driving
         """
         a = mid - left
         b = right - mid
         width = right - left
     
         if a >= b:  # driving right off
             offset = a / width * LANEWIDTH - LANEWIDTH /2.0
         else:       # driving left off
             offset = LANEWIDTH /2.0 - b / width * LANEWIDTH
     
         return offset
     ```

6. Lane augmentation and departure warning

   ​	마지막으로 감지된 차선 영역을 원래 입력 영상에 그린다. 차량이 차선 가장자리에 너무 가까워지면 색상을 녹색에서 빨간색으로 변경하여 운전자에게 경고한다. `lane.py` 에서 `create_output_frame` 라는 함수로 해당 내용이 구현되어 있다.

   ```python
   def create_output_frame(offcenter, pts, undist_ori, fps, curvature, curve_direction, binary_sub, threshold=0.6):
       """
   
       :param offcenter:
       :param pts:
       :param undist_ori:
       :param fps:
       :param threshold:
       :return:
       """
   
       undist_ori = cv2.resize(undist_ori, (0,0), fx=1/output_frame_scale, fy=1/output_frame_scale)
       w = undist_ori.shape[1]
       h = undist_ori.shape[0]
   
       undist_birdview = warper(cv2.resize(undist_ori, (0,0), fx=1/2, fy=1/2), M_b)
   
       color_warp = np.zeros_like(undist_ori).astype(np.uint8)
   
       # create a frame to hold every image
       whole_frame = np.zeros((math.ceil(h*2.5),math.ceil(w*2.34), 3), dtype=np.uint8)
   
       if abs(offcenter) > threshold:  # car is offcenter more than 0.6 m
           # Draw Red lane
           cv2.fillPoly(color_warp, np.int_([pts]), (255, 0, 0)) # red
       else: # Draw Green lane
           cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))  # green
   
       newwarp = cv2.warpPerspective(color_warp, M_inv, (int(frame_width/input_scale), int(frame_height/input_scale)))
   
       # Combine the result with the original image    # result = cv2.addWeighted(undist, 1, newwarp, 0.3, 0)
   
       newwarp_ = cv2.resize(newwarp,None, fx=input_scale/output_frame_scale, fy=input_scale/output_frame_scale, interpolation = cv2.INTER_LINEAR)
   
       output = cv2.addWeighted(undist_ori, 1, newwarp_, 0.3, 0)
   
       ############## generate the combined output frame only for visualization purpose ################
       whole_frame[40:40+h, 20:20+w, :] = undist_ori
       whole_frame[40:40+h, 60+w:60+2*w, :] = output
       whole_frame[220+h//2:220+2*h//2, 20:20+w//2, :] = undist_birdview
       whole_frame[220+h//2:220+2*h//2, 40+w//2:40+w, 0] = cv2.resize((binary_sub*255).astype(np.uint8), (0,0), fx=1/2, fy=1/2)
       whole_frame[220+h//2:220+2*h//2, 40+w//2:40+w, 1] = cv2.resize((binary_sub*255).astype(np.uint8), (0,0), fx=1/2, fy=1/2)
       whole_frame[220+h//2:220+2*h//2, 40+w//2:40+w, 2] = cv2.resize((binary_sub*255).astype(np.uint8), (0,0), fx=1/2, fy=1/2)
   
       font = cv2.FONT_HERSHEY_SIMPLEX
       if offcenter >= 0:
           offset = offcenter
           direction = 'Right'
       elif offcenter < 0:
           offset = -offcenter
           direction = 'Left'
   
       info_road = "Road Status"
       info_lane = "Lane info: {0}".format(curve_direction)
       info_cur = "Curvature {:6.1f} m".format(curvature)
       info_offset = "Off center: {0} {1:3.1f}m".format(direction, offset)
       info_framerate = "{0:4.1f} fps".format(fps)
       info_warning = "Warning: offcenter > 0.6m (use higher threshold in real life)"
   
       cv2.putText(whole_frame, "Departure Warning System with a Monocular Camera", (23,25), font, 0.8, (255,255,0), 1, cv2.LINE_AA)
       cv2.putText(whole_frame, "Origin", (22,70), font, 0.6, (255,255,0), 1, cv2.LINE_AA)
       cv2.putText(whole_frame, "Augmented", (40+w+25,70), font, 0.6, (255,255,0), 1, cv2.LINE_AA)
       cv2.putText(whole_frame, "Bird's View", (22+30,70+35+h), font, 0.6, (255,255,0), 1, cv2.LINE_AA)
       cv2.putText(whole_frame, "Lanes", (22+225,70+35+h), font, 0.6, (255,255,0), 1, cv2.LINE_AA)
       cv2.putText(whole_frame, info_road, (40+w+50,70+35+h), font, 0.8, (255,255,0), 1,cv2.LINE_AA)
       cv2.putText(whole_frame, info_warning, (35+w,60+h), font, 0.4, (255,255,0), 1,cv2.LINE_AA)
       cv2.putText(whole_frame, info_lane, (40+w+50,70+35+40+h), font, 0.8, (255,255,0), 1,cv2.LINE_AA)
       cv2.putText(whole_frame, info_cur, (40+w+50,70+35+80+h), font, 0.8, (255,255,0), 1,cv2.LINE_AA)
       cv2.putText(whole_frame, info_offset, (40+w+50,70+35+120+h), font, 0.8, (255,255,0), 1,cv2.LINE_AA)
       cv2.putText(whole_frame, info_framerate, (40+w+250,70), font, 0.6, (255,255,0), 1,cv2.LINE_AA)
   
       return whole_frame
   ```



#### Discussion

​	일단 이 소스는 영상 처리 기법만을 이용하여 단안 카메라에서 차선을 인식하고 경고를 주는 LDW 시스템이다. 소스 코드 내에서 각종 파라미터들을 제한된 환경에 맞게 하드코딩되어 있는 것도 있고, 차선 정확도도 실제 사용할만한 수준의 LDW 에 비해 떨어진다. 이 알고리즘은 대체로 맑은 날 낮, 또렷한 차선이 있는 고속도로에서 잘 동작할 것이다. 여기에서 좀 더 실제 사용 가능한 LDW 알고리즘을 구현하기 위해서는 아마 다음과 같은 두 가지 방법이 있을 것같다.

- 단안 카메라 외에 이용할 수 있는 다양한 센서를 이용한다. (라이다, 스테레오 카메라 등)

- 기본적인 영상 처리 기법 외 다양한 머신러닝 알고리즘을 적용하여, 정확도를 개선 (예를 들면, 통계적 데이터 접근 방법을 이용하여 차선 추적 정확도를 높인다던지 추후 논문 소개)

  

​	원래는 코드를 따라가며 이해한 내용을 쓰려했으나, 프로젝트의 readme가 상당히 정리가 잘 되어 있어 해당 내용을 읽으면서 이해한 내용에 이해가 어려워 추가 설명이 필요한 내용 등을 추가하였다. 코드 또한  이해가 쉽도록`lane.py` 내의 `process_frame()` 을 읽으면서 따라가 보면 대체로 직관적으로 작성되어 있다.



---

* 기타 메모

###### 한 장씩 lane detection 처리

```python
import os
from lane import *
from moviepy.editor import *
from natsort import natsorted, ns

if __name__ == '__main__' :
    flie_list = natsorted(os.listdir('test_pic/test_images/'), key=lamdba y: y.lower())
    
    for imagepath in file_list:
        img = cv2.imread('test_pic/' + imagepath)
        img_aug = process_frame(img)
        
        cv2.imwrite('test_pic/aug_images/' + imagepath + '.jpg', img_aug)
```
